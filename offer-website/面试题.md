### 1. junit用法，before,beforeClass,after, afterClass的执行顺序

执行顺序是：

     @BeforeClass ，@Before，@Test，@After，@AfterClass




> @BeforeClass 在**本类加载前**执行，注意的是有关键字：static
> 
>  @Before  在执行目标测试**方法前**执行
>  
>  @Test **目标测试方法**
>  
>  @After  在执行目标测试**方法后**执行
>  
>  @AfterClass 在**本类加载后**执行，注意的是有关键字：static
  


### 2. 分布式锁

#### 分布式锁是控制分布式系统之间同步访问共享资源的一种方式。

分布式锁的方法有哪些。



> 1、使用数据库乐观锁，包括主键防重，版本号控制。但是这两种方法各有利弊。

使用主键冲突的策略进行防重，在并发量非常高的情况下对数据库性能会有影响，尤其是应用数据表和主键冲突表在一个库的时候，表现更加明显。其实针对是否会对数据库性能产生影响这个话题，我也和一些专业的DBA同学讨论过，普遍认可的是在MySQL数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象，比较好的办法是在程序中生产主键进行防重。

使用版本号策略 
这个策略源于mysql的mvcc机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断sql每次进行判断。


> 2、Zookeeper防重策略

ZooKeeper客户端curator的分布式锁实现

ZooKeeper版本的分布式锁问题相对比较来说少。
锁的占用时间限制：redis就有占用时间限制，而ZooKeeper则没有，最主要的原因是redis目前没有办法知道已经获取锁的客户端的状态，是已经挂了呢还是正在执行耗时较长的业务逻辑。而ZooKeeper通过临时节点就能清晰知道，如果临时节点存在说明还在执行业务逻辑，如果临时节点不存在说明已经执行完毕释放锁或者是挂了。由此看来redis如果能像ZooKeeper一样添加一些与客户端绑定的临时键，也是一大好事。
是否单点故障：redis本身有很多中玩法，如客户端一致性hash，服务器端sentinel方案或者cluster方案，**很难做到一种分布式锁方式能应对所有这些方案**。而ZooKeeper只有一种玩法，多台机器的节点数据是一致的，没有redis的那么多的麻烦因素要考虑。
总体上来说ZooKeeper实现分布式锁更加的简单，可靠性更高。


> 3 ，redis 实现分布式锁

`setnx` 来创建一个 key，如果key不存在则创建成功返回1，如果key已经存在则返回0。依照上述来判定是否获取到了锁
获取到锁的执行业务逻辑，完毕后删除lock_key，来实现释放锁
其他未获取到锁的则进行不断重试，直到自己获取到了锁

上述逻辑在正常情况下是OK的，但是一旦获取到锁的客户端挂了，没有执行上述释放锁的操作，则其他客户端就无法获取到锁了，所以在这种情况下有2种方式来解决：

- 为lock_key设置一个过期时间
- 对lock_key的value进行判断是否过期

### 3. nginx的请求转发算法，如何配置根据权重转发
####  Nginx负载均衡算法
    
	1、轮询（默认）
        每个请求按时间顺序逐一分配到不同的后端服务，如果后端某台服务器死机，自动剔除故障系统，使用户访问不受影响。
    
	2、weight（轮询权值）
        weight的值越大分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。或者仅仅为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。
    
	3、ip_hash
        每个请求按访问IP的哈希结果分配，使来自同一个IP的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的session共享问题。
    
	4、fair
        比 weight、ip_hash更加智能的负载均衡算法，fair算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间 来分配请求，响应时间短的优先分配。Nginx本身不支持fair，如果需要这种调度算法，则必须安装upstream_fair模块。
    
	5、url_hash
        按访问的URL的哈希结果来分配请求，使每个URL定向到一台后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身不支持url_hash，如果需要这种调度算法，则必须安装Nginx的hash软件包。

#### Nginx负载均衡调度状态

  在Nginx upstream模块中，可以设定每台后端服务器在负载均衡调度中的状态，常用的状态有：
    
	1、down，表示当前的server暂时不参与负载均衡
    
	2、backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的访问压力最低
    
	3、max_fails，允许请求失败的次数，默认为1，当超过最大次数时，返回proxy_next_upstream 模块定义的错误。

    4、fail_timeout，请求失败超时时间，在经历了 max_fails 次失败后，暂停服务的时间。max_fails 和 fail_timeout 可以一起使用。

#### Nginx负载均衡配置

在Nginx配置文件的HTTP层添加upstream模块

```
http {
upstream webserver {
    server 192.168.1.209:80 weight=2 max_fails=3 fail_timeout=10s;
    server 192.168.1.250:80 weight=1 max_fails=3 fail_timeout=10s;
}
server {
    listen       80;
    server_name  www.huangming.org 192.168.1.21;
    index index.html index.htm index.php index.jsp;
 
        location / {
        proxy_pass http://webserver;
        proxy_set_header Host   $host;
        proxy_set_header X-Real-IP      $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_next_upstream http_500 http_502 http_503 error timeout invalid_header;
    }
```
 
 


### 4. 用hashmap实现redis有什么问题

[http://ifeve.com/concurrenthashmap/](http://ifeve.com/concurrenthashmap/ "http://ifeve.com/concurrenthashmap/")

	（死锁，死循环，可用 ConcurrentHashmap）

多线程环境下，使用Hashmap进行put操作会引起死循环，导致CPU利用率接近100%，所以在并发情况下不能使用HashMap。



#### 效率低下的HashTable容器
  
>    HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下
>    HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，
>    其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。
>    如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，
>    并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。

#### ConcurrentHashMap的锁分段技术
    

> HashTable容器在竞争激烈的并发环境下表现出效率低下的原因，是因为所有访问HashTable的线程都必须竞争同一把锁，
> 那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，

> 那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提

> 高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，
> 
> 然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。

#### ConcurrentHashMap segment 结构

`ConcurrentHashMap` 是由 `Segment` 数组结构 和 `HashEntry` 数组结构组成。

`Segment` 是一种可重入锁 `ReentrantLock`，在 `ConcurrentHashMap` 里扮演**锁**的角色

`HashEntry` 则用于存储键值对数据。

一个 `ConcurrentHashMap` 里包含一个 `Segment` 数组，`Segment` 的结构和`HashMap` 类似，是一种**数组和链表**结构，

一个 `Segment` 里包含一个 `HashEntry` 数组，

每个 `HashEntry` 是一个链表结构的元素， 

每个 `Segment` 守护着一个 `HashEntry` 数组里的元素

当对 `HashEntry` 数组的数据进行修改时，必须首先获得它对应的 `Segment` 锁。


既然ConcurrentHashMap使用分段锁Segment来保护不同段的数据，那么在插入和获取元素的时候，必须先通过哈希算法定位到Segment。可以看到ConcurrentHashMap会首先使用Wang/Jenkins hash的变种算法对元素的hashCode进行一次再哈希。

```
private static int hash(int h) {
 
h += (h << 15) ^ 0xffffcd7d; h ^= (h >>> 10);
 
h += (h << 3); h ^= (h >>> 6);
 
h += (h << 2) + (h << 14); return h ^ (h >>> 16);
 
}
```

再哈希，其目的是为了减少哈希冲突，使元素能够均匀的分布在不同的Segment上，从而提高容器的存取效率。

#### ConcurrentHashMap的get操作
Segment的get操作实现非常简单和高效。先经过一次再哈希，然后使用这个哈希值通过哈希运算定位到segment，再通过哈希算法定位到元素，代码如下：

```java
public V get(Object key) {

       int hash = hash(key.hashCode());

       return segmentFor(hash).get(key, hash);
   }
```

get操作的高效之处在于整个get过程不需要加锁，除非读到的值是空的才会加锁重读，我们知道HashTable容器的get方法是需要加锁的，那么ConcurrentHashMap的get操作是如何做到不加锁的呢？

原因是它的get方法里将要使用的共享变量都定义成volatile，如用于统计当前Segement大小的count字段和用于存储值的HashEntry的value。

定义成volatile的变量，能够在线程之间保持可见性，能够被多线程同时读，并且保证不会读到过期的值，

但是只能被单线程写（有一种情况可以被多线程写，就是写入的值不依赖于原值），在get操作里只需要读不需要写共享变量count和value，所以可以不用加锁。

之所以不会读到过期的值，是根据java内存模型的happen before原则，对volatile字段的写入操作先于读操作，即使两个线程同时修改和获取volatile变量，get操作也能拿到最新的值，这是用volatile替换锁的经典应用场景。

```java
transient volatile int count;

volatile V value;
```

在定位元素的代码里我们可以发现定位HashEntry和定位Segment的哈希算法虽然一样，都与数组的长度减去一相与，但是相与的值不一样，定位Segment使用的是元素的hashcode通过再哈希后得到的值的高位，而定位HashEntry直接使用的是再哈希后的值。其目的是避免两次哈希后的值一样，导致元素虽然在Segment里散列开了，但是却没有在HashEntry里散列开。

```java
hash >>> segmentShift) & segmentMask//定位Segment所使用的hash算法

int index = hash & (tab.length - 1);// 定位HashEntry所使用的hash算法
```

#### ConcurrentHashMap的Put操作

由于put方法里需要对共享变量进行写入操作，所以为了线程安全，在操作共享变量时必须得加锁。Put方法首先定位到Segment，然后在Segment里进行插入操作。插入操作需要经历两个步骤，

第一步判断是否需要对Segment里的HashEntry数组进行扩容，

第二步定位添加元素的位置然后放在HashEntry数组里。

**是否需要扩容**。在插入元素前会先判断Segment里的HashEntry数组是否超过容量（threshold），如果超过阀值，数组进行扩容。值得一提的是，Segment的扩容判断比HashMap更恰当，因为HashMap是在插入元素后判断元素是否已经到达容量的，如果到达了就进行扩容，但是很有可能扩容之后没有新元素插入，这时HashMap就进行了一次无效的扩容。

如何扩容。

扩容的时候首先会创建一个两倍于原容量的数组，然后将原数组里的元素进行再hash后插入到新的数组里。为了高效ConcurrentHashMap不会对整个容器进行扩容，而只对某个segment进行扩容。

#### ConcurrentHashMap的size操作

如果我们要统计整个ConcurrentHashMap里元素的大小，就必须统计所有Segment里元素的大小后求和。Segment里的全局变量count是一个volatile变量，那么在多线程场景下，我们是不是直接把所有Segment的count相加就可以得到整个ConcurrentHashMap大小了呢？不是的，虽然相加时可以获取每个Segment的count的最新值，但是拿到之后可能累加前使用的count发生了变化，那么统计结果就不准了。所以最安全的做法，是在统计size的时候把所有Segment的put，remove和clean方法全部锁住，但是这种做法显然非常低效。

因为在累加count操作过程中，之前累加过的count发生变化的几率非常小，所以ConcurrentHashMap的做法是先尝试2次通过不锁住Segment的方式来统计各个Segment大小，如果统计的过程中，容器的count发生了变化，则再采用加锁的方式来统计所有Segment的大小。

那么ConcurrentHashMap是如何判断在统计的时候容器是否发生了变化呢？使用modCount变量，在put , remove和clean方法里操作元素前都会将变量modCount进行加1，那么在统计size前后比较modCount是否发生变化，从而得知容器的大小是否发生变化。



### 5. 线程的状态

![](http://i.imgur.com/picRr7N.gif)


### 6. 线程的阻塞的方式


### 7. sleep和wait的区别

sleep是**Thread类的静态方法**。sleep的作用是让线程休眠制定的时间，在时间到达时恢复，也就是说sleep将在接到时间到达事件事恢复线程执行

sleep()是让某个线程暂停运行一段时间,其控制范围是由当前线程决定

wait是**Object的方法**。 也就是说可以对任意一个对象调用wait方法，调用wait方法将会将调用者的线程挂起，直到其他线程调用**同一个对象**的notify方法才会重新激活调用者

由某个确定的对象来调用的

```
	1，这两个方法来自不同的类分别是Thread和Object
  	
	2，最主要是sleep方法没有释放锁，而wait方法释放了锁，使得其他线程可以使用同步控制块或者方法。
  	
	3，wait，notify和notifyAll只能在同步控制方法或者同步控制块里面使用，而sleep可以在
    任何地方使用
   	synchronized(x){
      x.notify()
     //或者wait()
  	 }
	
	4,sleep必须捕获异常，而wait，notify和notifyAll不需要捕获异常
```	


### 8. hashmap的底层实现

Hashmap实际上是一个 **数组** 和 **链表** 的结合体（在数据结构中，一般称之为“链表散列“）

  Hashmap里面的bucket出现了单链表的形式，散列表要解决的一个问题就是散列值的冲突问题，通常是两种方法：

**链表法**和**开放地址法**。

链表法就是将相同hash值的对象组织成一个链表放在hash值对应的槽位；

HashMap其实就是一个Entry数组，Entry对象中包含了键和值，其中next也是一个Entry对象，它就是用来处理hash冲突的，形成一个链表。


开放地址法是通过一个探测算法，当某个槽位已经被占据的情况下继续查找下一个可以使用的槽位。

一般比较常用的方法有开放地址法：（内容来自百度百科） 
1. 开放寻址法：Hi=(H(key) + di) MOD m,i=1,2，…，k(k<=m-1），其中H(key）为散列函数，m为散列表长，di为增量序列，可有下列三种取法： 
1.1. di=1,2,3，…，m-1，称线性探测再散列；顺序查看表的下一单元，直至找到某个空单元，或查遍全表。 
1.2. di=1^2,-1^2,2^2,-2^2，⑶^2，…，±（k)^2,(k<=m/2）称二次探测再散列；在表的左右进行跳跃式探测。
1.3. di=伪随机数序列，称伪随机探测再散列。根据产生的随机数进行探测。

2 再散列法：建立多个hash函数，若是当发生hash冲突的时候，使用下一个hash函数，直到找到可以存放元素的位置。

3 拉链法（链地址法）：就是在冲突的位置上简历一个链表，然后将冲突的元素插入到链表尾端，

4 建立公共溢出区：将哈希表分为基本表和溢出表，将与基本表发生冲突的元素放入溢出表中。

底层的hashMap是由数组和链表来实现的，就是上面说的拉链法。首先当插入的时候，会根据key的hash值然后计算出相应的数组下标，计算方法是index = hashcode%table.length，（这个下标就是 bucket），当这个下标上面已经存在元素的时候那么就会形成链表，将后插入的元素放到尾端，若是下标上面没有存在元素的话，那么将直接将元素放到这个位置上。 







### 9. 一万个人抢100个红包，如何实现（不用队列），如何保证2个人不能抢到同一个红包，可用分布式锁

利用redis + lua解决抢红包高并发的问题[http://blog.csdn.net/hengyunabc/article/details/19433779](http://blog.csdn.net/hengyunabc/article/details/19433779 "http://blog.csdn.net/hengyunabc/article/details/19433779")




### 9. java内存模型，垃圾回收机制，不可达算法


> java内存模型

程序计数器
多线程时，当线程数超过CPU数量或CPU内核数量，线程之间就要根据时间片轮询抢夺CPU时间资源。因此每个线程有要有一个独立的程序计数器，记录下一条要运行的指令。线程私有的内存区域。如果执行的是Java方法，计数器记录正在执行的java字节码地址，如果执行的是native方法，则计数器为空。

虚拟机栈
线程私有的，与线程在同一时间创建。管理JAVA方法执行的内存模型。每个方法执行时都会创建一个桢栈来存储方法的的变量表、操作数栈、动态链接方法、返回值、返回地址等信息。栈的大小决定了方法调用的可达深度（递归多少层次，或嵌套调用多少层其他方法，-Xss参数可以设置虚拟机栈大小）。栈的大小可以是固定的，或者是动态扩展的。如果请求的栈深度大于最大可用深度，则抛出stackOverflowError；如果栈是可动态扩展的，但没有内存空间支持扩展，则抛出OutofMemoryError。

本地方法区
和虚拟机栈功能相似，但管理的不是JAVA方法，是本地方法，本地方法是用C实现的。

**JAVA堆**
线程共享的，存放所有对象实例和数组。垃圾回收的主要区域。可以分为新生代和老年代(tenured)。
新生代用于存放刚创建的对象以及年轻的对象，如果对象一直没有被回收，生存得足够长，老年对象就会被移入老年代。
新生代又可进一步细分为eden、survivorSpace0(s0,from space)、survivorSpace1(s1,to space)。刚创建的对象都放入eden,s0和s1都至少经过一次GC并幸存。如果幸存对象经过一定时间仍存在，则进入老年代(tenured)。

**方法区**
线程共享的，用于存放被虚拟机加载的**类的元数据信息：如常量、静态变量、即时编译器编译后的代码**。也成为**永久代**。如果hotspot虚拟机确定一个类的定义信息不会被使用，也会将其回收。回收的基本条件至少有：所有该类的实例被回收，而且装载该类的ClassLoader被回收



> 垃圾回收机制

[https://ja0ck5.github.io/2016/12/27/JVM-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86/](https://ja0ck5.github.io/2016/12/27/JVM-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86/ "https://ja0ck5.github.io/2016/12/27/JVM-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86/")


**标记-清除算法(Mark-Sweep)**

从根节点开始标记所有可达对象，其余没标记的即为垃圾对象，执行清除。但回收后的空间是不连续的。

**复制算法(copying)**

将内存分成两块，每次只使用其中一块，垃圾回收时，将标记的对象拷贝到另外一块中，然后完全清除原来使用的那块内存。复制后的空间是连续的。复制算法适用于新生代，因为垃圾对象多于存活对象，复制算法更高效。在新生代串行垃圾回收算法中，将eden中标记存活的对象拷贝未使用的s1中，s0中的年轻对象也进入s1，如果s1空间已满，则进入老年代；这样交替使用s0和s1。这种改进的复制算法，既保证了空间的连续性，有避免了大量的内存空间浪费。

**标记-压缩算法(Mark-compact)**

适合用于老年代的算法（存活对象多于垃圾对象）。
标记后不复制，而是将存活对象压缩到内存的一端，然后清理边界外的所有对象。


**JVM参数：**

-XX:+PrintGCDetails  打印垃圾回收信息

-Xms 为Heap区域的初始值，线上环境需要与-Xmx设置为一致，否则capacity的值会来回飘动
-Xmx 为Heap区域的最大值
-Xss（或-ss） 线程栈大小（指一个线程的native空间）1.5以后是1M的默认大小
-XX:PermSize与-XX:MaxPermSize  方法区（永久代）的初始大小和最大值（但不是本地方法区）
-XX:NewRatio  老年代与新生代比率
-XX:SurvivorRatio  Eden与Survivor的占用比例。例如8表示，一个survivor区占用 1/8 的Eden内存，即1/10的新生代内存，为什么不是1/9？因为我们的新生代有2个survivor，即S1和S22。所以survivor总共是占用新生代内存的 2/10，Eden与新生代的占比则为 8/10。
-XX:MaxHeapFreeRatio  GC后，如果发现空闲堆内存占到整个预估的比例小于这个值，则减小堆空间。
-XX:MinHeapFreeRatio  GC后，如果发现空闲堆内存占到整个预估的比例大于这个值，则增大堆空间。
-XX:NewSize    新生代大小



### 10. 两个Integer的引用对象传给一个swap方法在方法内部交换引用，返回后，两个引用的值是否会发现变化

不会。



11. aop的底层实现，动态代理是如何动态，假如有100个对象，如何动态的为这100个对象代理

`InvocationHandler`：每一个动态代理类都必须实现 `InvocationHandler` 这个接口，并且每个代理类的实例都关联到了一个 `handler`，当我们通过代理对象调用一个方法的时候，这个方法的调用就会被转发为由 `InvocationHandler` 这个接口的 `invoke` 方法来进行调用。

```java
Object invoke(Object proxy,Method method,Object[] args)
```

	`proxy`：指代我们所代理的真实对象
	
	`method`：指代的是我们所要调用真实对象的某个方法的Method对象
	
	`args`：指代的是调用真实对象某个方法时接收的参数

`Proxy`：这个类的作用就是用来动态的创建一个代理对象的类，其接受三个参数

```java
public static Object newProxyInstance(ClassLoader loader,Class<?>[] interfaces,InvocationHandler h) throws IllegalArgumentException
```

	loader：一个ClassLoader对象，定义了由哪个ClassLoader对象来生成的代理对象进行加载

	interfaces：一个Interface对象的数组，表示的是我将要给我需要代理的对象提供一组接口，如果我提供了一组接口给它，那么这个代理对象就宣称实现了该接口（多态），这样我就能调用这组接口中的方法了。

	h：一个InvocationHandler对象，表示的是当我这个动态代理在调用方法的时候，会关联到哪一个InvocationHandler对象上

**动态代理的作用**：

主要用来做方法的增强，让你可以在不修改源码的情况下，增强一些方法，在方法执行前后做任何你想做的事情（甚至根本不去执行这个方法），因为在InvocationHandler的invoke这个方法中，你可以直接获取正在调用方法对应的Method对象，具体应用，比如，添加日志，做事务控制等。

### 12. 是否用过maven install。 maven test。git（make install是安装本地jar包）


### 13. tomcat的各种配置，如何配置docBase


### 14. spring的bean配置的几种方式
### 15. web.xml的配置
### 16. spring的监听器。
### 17. zookeeper的实现机制，有缓存，如何存储注册服务的
### 18. IO会阻塞吗？readLine是不是阻塞的

read方法和readLine方法在任何情况下都是阻塞的

### 19. 用过spring的线程池还是java的线程池？


### 20. 字符串的格式化方法 （20，21这两个问题问的太低级了）

```java
public static void main(String[] args) {  
    String str=null;  
    str=String.format("Hi,%s", "王力");  
    System.out.println(str);  
    str=String.format("Hi,%s:%s.%s", "王南","王力","王张");            
    System.out.println(str);                           
    System.out.printf("字母a的大写是：%c %n", 'A');  
    System.out.printf("3>7的结果是：%b %n", 3>7);  
    System.out.printf("100的一半是：%d %n", 100/2);  
    System.out.printf("100的16进制数是：%x %n", 100);  
    System.out.printf("100的8进制数是：%o %n", 100);  
    System.out.printf("50元的书打8.5折扣是：%f 元%n", 50*0.85);  
    System.out.printf("上面价格的16进制数是：%a %n", 50*0.85);  
    System.out.printf("上面价格的指数表示：%e %n", 50*0.85);  
    System.out.printf("上面价格的指数和浮点数结果的长度较短的是：%g %n", 50*0.85);  
    System.out.printf("上面的折扣是%d%% %n", 85);  
    System.out.printf("字母A的散列码是：%h %n", 'A');  
}  
```



### 21. 时间的格式化方法

			Date nowTime=new Date(); 
        	System.out.println(nowTime); 
        	SimpleDateFormat sdf=new SimpleDateFormat("yyyy MM dd HH mm ss"); 
        	String format = sdf.format(nowTime);


### 22. 定时器用什么做的

### 23. 线程如何退出结束

    1.  使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。 
    
    2.  使用stop方法强行终止线程（这个方法不推荐使用，因为stop和suspend、resume一样，也可能发生不可预料的结果）。  使用stop方法可以强行终止正在运行或挂起的线程。

	使用 stop() 来退出线程是不安全的。它会解除由线程获取的所有锁，可能导致数据不一致。
    
    3.  使用interrupt方法中断线程
    
	4.  异常法退出线程
    

### 24. java有哪些锁？
 
     乐观锁 悲观锁 synchronized 可重入锁 读写锁

	用过reentrantlock吗？

	reentrantlock与synmchronized的区别

	

> 相似之处，它们都是加锁方式同步，而且都是阻塞式的同步。
	
	也就是说当如果一个线程获得了对象锁，进入了同步块，其他访问该同步块的线程都必须阻塞在同步块外面等待。

	而进行线程阻塞和唤醒的代价是比较高的（操作系统需要在用户态与内核态之间来回切换，代价很高，不过可以通过对锁优化进行改善）。


> 区别：
      
	  这两种方式最大区别就是对于Synchronized来说，它是java语言的关键字，是原生语法层面的互斥，需要jvm实现。


	  而ReentrantLock它是JDK 1.5之后提供的API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成。

`Synchronized` 进过编译，会在同步块的前后分别形成 `monitorenter` 和 `monitorexit` 这个两个字节码指令。在执行 `monitorenter` 指令时，首先要尝试获取对象锁。

如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行 `monitorexit` 指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。

如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。



`ReentrantLock`

   由于 `ReentrantLock` 是 `java.util.concurrent` 包下提供的一套互斥锁，相比 `Synchronized`，`ReentrantLock` 类提供了一些高级功能，主要有以下3项：
        1.等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。

        2.公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized 锁非公平锁，ReentrantLock 默认的构造函数是创建的非公平锁，可以通过参数 true 设为公平锁，但公平锁表现的性能不是很好。

        3.锁绑定多个条件，一个 ReentrantLock 对象可以同时绑定对个对象。

   ReentrantLock在采用非公平锁构造时，首先检查锁状态，如果锁可用，直接通过CAS设置成持有状态，且把当前线程设置为锁的拥有者。
如果当前锁已经被持有，那么接下来进行可重入检查，如果可重入，需要为锁状态加上请求数。如果不属于上面两种情况，那么说明锁是被其他线程持有，

当前线程应该放入等待队列。
     在放入等待队列的过程中，首先要检查队列是否为空队列，如果为空队列，需要创建虚拟的头节点，然后把对当前线程封装的节点加入到队列尾部。由于设置尾部节点采用了CAS，为了保证尾节点能够设置成功，这里采用了无限循环的方式，直到设置成功为止。
     在完成放入等待队列任务后，则需要维护节点的状态，以及及时清除处于Cancel状态的节点，以帮助垃圾收集器及时回收。如果当前节点之前的节点的等待状态小于1，说明当前节点之前的线程处于等待状态(挂起)，那么当前节点的线程也应处于等待状态(挂起)。挂起的工作是由LockSupport类支持的，LockSupport通过JNI调用本地操作系统来完成挂起的任务(java中除了废弃的suspend等方法，没有其他的挂起操作)。
    在当前等待的线程，被唤起后，检查中断状态，如果处于中断状态，那么需要中断当前线程。

### 25. ThreadLocal的使用场景

	单点登录中用于用户登录后保存用户对象。

Spring的事务管理器通过AOP切入业务代码，在进入业务代码前，会根据对应的事务管理器提取出相应的事务对象，假如事务管理器是DataSourceTransactionManager，就会从DataSource中获取一个连接对象，通过一定的包装后将其保存在ThreadLocal中。并且Spring也将DataSource进行了包装，重写了其中的getConnection()方法，或者说该方法的返回将由Spring来控制，这样Spring就能让线程内多次获取到的Connection对象是同一个。
	
	

### 26. java的内存模型，垃圾回收机制
### 27. 为什么线程执行要调用start而不是直接run
 （直接run，跟普通方法没什么区别，先调start，run才会作为一个线程方法运行）

### 28. qmq消息的实现机制(qmq是去哪儿网自己封装的消息队列)

### 29. 遍历hashmap的三种方式

### 30. jvm的一些命令



### 31. memcache和redis的区别

> Memcache与Redis的比较
> 
> （1）数据结构：Memcache只支持key value存储方式，Redis支持更多的数据类型，比如Key value，hash，list，set，zset；
> 
> （2）多线程：Memcache支持多线程，redis支持单线程；CPU利用方面Memcache优于redis；
> 
> （3）持久化：Memcache不支持持久化，Redis支持持久化；
> 
> （4）内存利用率：memcache高，redis低（采用压缩的情况下比memcache高）；
> 
> （5）过期策略：memcache过期后，不删除缓存，会导致下次取数据数据的问题，Redis有专门线程，清除缓存数据；
> 

### 32. mysql的行级锁加在哪个位置


####行级锁

行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为 共享锁 和 排他锁 。

特点开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。

#### 表级锁

表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为 读锁 与 写锁 。

特点开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。

#### 页级锁

是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁.表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。BDB支持页级锁.

特点开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般.

#### MyISAM锁细述

(1). 锁模式

`MySQL` 的表级锁有两种模式： **表共享读锁**（`Table Read Lock`）和 **表独占写锁**（`Table Write Lock`）。

(2).  如何加锁

当MyISAM在执行**查询**语句时，会自动给涉及到表**加读锁**，在执行**更新**操作时，会**加写锁**。

当然用户也可以用 `LOCK TABLE` 去显式的加锁。 

显式的加锁一般是应用于：需要在一个时间点实现多个表的一致性读取，

不然的话，可能读第一个表时，其他表由于还没进行读操作，没有自动加锁，可能数据会发生改变。

并且**显示加锁后只能访问加锁的表，不能访问其他表**。

(3). 并发插入

`MyISAM` 存储引擎有个系统变量 `concurrent_insert`，专门用来控制并发插入的行为，可以取 0 ， 1 ， 2。

**0** 表示不允许并发插入，

**1** 表示表中间没有删除的行时可以在表末尾插入，

**2** 表示总是可以插入。

一般如果对并发要求比较高的情况下，可以设置为2，总是可以插入，然后定期在数据库空闲时间对表进行optimize。

(4). 锁的调度

需要注意的是，其中读操作不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；并且当写锁和读锁同时被申请时，优先获得写锁，这也这正是表级锁发生锁冲突概率最高的原因，因为写锁可能会一直阻塞读锁，所以不适合有大量写操作的环境下工作。这一问题可以通过设置low-priority-updates这一启动参数来降低写的优先级。
虽然写锁优先于读锁获取，但是长时间的查询操作也可能会让写操作饿死，所以尽量避免一条SQL语句执行所有的查询，应该进行必要的分解。




#### InnoDB锁细述

由于InnoDB支持事务，并默认是使用行级锁，所以InnoDB的锁问题和MyISAM锁问题还是有蛮大差别的。

(1). 锁模式

共享锁(S)和排他锁(X)，分别类似于MyISAM的读锁和写锁。对于 UPDATE、 DELETE 和 INSERT 语句，InnoDB会自动给涉及数据集加排他锁（X)；对于普通 SELECT 语句，InnoDB不会加任何锁。

(2). 如何加锁

可以显式的加锁，用lock in share mode 显式的加共享锁，用 for update 显式的加排他锁。

需要注意的是，如果线程A加了共享锁后，线程B对同一个表加了共享锁，那么两个线程需要进行更新操作时会产生死锁。所以，进行更新操作时最好加排他锁。

(3). InnoDB行锁的实现方式——索引加锁

这一点与Oracle不同，所以这也意味着(重要)：

 1. 只有通过索引条件检索数据时，InnoDB才会使用行级锁，否则会使用表级锁。
 2. 即使是访问不同行的记录，如果使用的是相同的索引键，会发生锁冲突。
 3. 如果数据表建有多个索引时，可以通过不同的索引锁定不同的行。

(4). 间隙锁

InnoDB支持事务，为了满足隔离级别的要求，InnoDB有个间隙锁，当使用范围查找时，InnoDB会给满足key范围要求，但实际并不存在的记录加锁。例如：select * from user where id > 100 for updata 会给ID>100的记录加排他锁，满足这个范围，但不存在的记录，会加间隙锁，这样可以避免幻读，避免读取的时候插入满足条件的记录。

(5). 隔离级别与锁

一般来说，隔离级别越高，加锁就越严格。这样，产生锁冲突的概率就越大，一般实际应用中，通过优化应用逻辑，选用 可提交读 级别就够了。对于一些确实需要更高隔离级别的事务，再通过set session transaction isolation level+"级别" 来动态改变满足需求。

死锁

MyISAM是没有死锁问题的，因为他会一次性获得所有的锁。InnoDB发生死锁后一般能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。

在应用中，可以通过如下方式来尽可能的避免死锁：

(1) 如果不同的程序会并发的存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会。

(2) 在程序以批量方式处理数据时，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大的降低出现死锁的可能。



### 33. ConcurrentHashmap的锁是如何加的？是不是分段越多越好


### 34. myisam和innodb的区别

 （innodb是行级锁，myisam是表级锁）


### 35. mysql其他的性能优化方式

第一优化你的sql和索引；

第二加缓存，memcached,redis；

第三以上都做了后，还是慢，就做主从复制或双主热备，读写分离，可以在应用层做，效率高，也可以用三方工具，第三方工具推荐360的atlas,其它的要么效率不高，要么没人维护；

第四如果以上都做了还是慢，不要想着去做切分，mysql自带分区表，先试试这个，对你的应用是透明的，无需更改代码,但是sql语句是需要针对分区表做优化的，sql条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区

第五如果以上都做了，那就先做垂直拆分，其实就是根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；

第六才是水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key,为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表；


### 36. linux系统日志在哪里看

	/var/log/messages

### 37. 如何查看网络进程

	netstat -aunpt |grep CONNECTED

38. 统计一个整数的二进制表示中bit为1的个数

```java
public int numberOf1(int n) {
		int count = 0;
		while (n != 0) {
			count++;
			n &= (n - 1);
		}
		return count;
	}
```


### 40. 如何把java内存的数据全部dump出来

查看整个JVM内存状态 

```
jmap -heap [pid]
```

要注意的是在使用CMS GC 情况下，jmap -heap的执行有可能会导致JAVA 进程挂起

查看JVM堆中对象详细占用情况

```
jmap -histo [pid]
```

导出整个JVM 中内存信息

```
jmap -dump:format=b,file=文件名 [pid]
```
jhat是sun 1.6及以上版本中自带的一个用于分析JVM 堆DUMP 文件的工具，基于此工具可分析JVM HEAP 中对象的内存占用情况

```
jhat -J-Xmx1024M [file]
```

执行后等待console 中输入start HTTP server on port 7000 即可使用浏览器访问 IP：7000

### 41. 如何手动触发全量回收垃圾，如何立即触发垃圾回收

System.gc();



42. hashmap如果只有一个写其他全读会出什么问题

43. git rebase

44. mongodb和hbase的区别

45. 如何解决并发问题


### 46. volatile的用途

ConcurrentHashMap  get方法里将要使用的共享变量都定义成volatile，如用于统计当前Segement大小的count字段和用于存储值的HashEntry的value。

定义成volatile的变量，能够在线程之间保持可见性，能够被多线程同时读，并且保证不会读到过期的值。

对于volatile关键字，当且仅当满足以下所有条件时可使用：

1. 对变量的写入操作不依赖变量的当前值，或者你能确保只有单个线程更新变量的值。
2. 该变量没有包含在具有其他变量的不变式中。

** `volatile` 和 `synchronized` 的区别**

volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。

volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的

volatile仅能实现变量的修改可见性，不能保证原子性；而synchronized则可以保证变量的修改可见性和原子性

volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。

volatile标记的变量不会被编译器优化；synchronized标记的变量可以被编译器优化



### 47. java线程池
### 48. mysql的binlog

二进制日志，也成为二进制日志，记录对数据发生或潜在发生更改的SQL语句，并以二进制的形式保存在磁盘中；

可以用来查看数据库的变更历史（具体的时间点所有的SQL操作）、数据库增量备份和恢复（增量备份和基于时间点的恢复）、Mysql的复制（主主数据库的复制、主从数据库的复制）

### 49. 代理模式

### 50. mysql是如何实现事务的
Redo Log

在Innodb存储引擎中，事务日志是通过redo和innodb的存储引擎日志缓冲（Innodb log buffer）来实现的。

**开始事务的时候**，会记录该事务的`lsn(log sequence number)`号;

**当事务执行时**，会往InnoDB存储引擎的日志的日志缓存里面插入事务日志；

**当事务提交时**，必须将存储引擎的日志缓冲写入磁盘（通过innodb_flush_log_at_trx_commit来控制），也就是写数据前，需要先写日志。这种方式称为“预写日志方式”，

 innodb通过此方式来保证事务的完整性。也就意味着磁盘上存储的数据页和内存缓冲池上面的页是不同步的，是先写入redo log，然后写入data file，因此是一种异步的方式。

通过 show engine innodb status\G 来观察之间的差距

### 51. 读写分离何时强制要读主库，读哪个从库是通过什么方式决定的，从库的同步mysql用的什么方式(通过主库发送来的binlog恢复数据)

	1.物理服务器增加，负荷增加
	2.主从只负责各自的写和读，极大程度的缓解X锁和S锁争用
	3.从库可配置myisam引擎，提升查询性能以及节约系统开销
	4.从库同步主库的数据和主库直接写还是有区别的，通过主库发送来的binlog恢复数据，但是，最重要区别在于主库向从库发送binlog是异步的，从库恢复数据也是异步的
	5.读写分离适用与读远大于写的场景，如果只有一台服务器，当select很多时，update和delete会被这些select访问中的数据堵塞，等待select结束，并发性能不高。 对于写和读比例相近的应用，应该部署双主相互复制
	
	6.可以在从库启动是增加一些参数来提高其读的性能，例如--skip-innodb、--skip-bdb、--low-priority-updates以及--delay-key-write=ALL。当然这些设置也是需要根据具体业务需求来定得，不一定能用上
	
	7.分摊读取。假如我们有1主3从，不考虑上述1中提到的从库单方面设置，假设现在1分钟内有10条写入，150条读取。那么，1主3从相当于共计40条写入，而读取总数没变，因此平均下来每台服务器承担了10条写入和50条读取（主库不承担读取操作）。因此，虽然写入没变，但是读取大大分摊了，提高了系统性能。另外，当读取被分摊后，又间接提高了写入的性能。所以，总体性能提高了，说白了就是拿机器和带宽换性能。MySQL官方文档中有相关演算公式：官方文档 见6.9FAQ之“MySQL复制能够何时和多大程度提高系统性能”
	
	8.MySQL复制另外一大功能是增加冗余，提高可用性，当一台数据库服务器宕机后能通过调整另外一台从库来以最快的速度恢复服务，因此不能光看性能，也就是说1主1从也是可以的。


### 52. mysql的存储引擎

53. mysql的默认隔离级别，其他隔离级别

MySQL默认使用可重复读（Read-Repeatable，RR)

SERIALIZABLE > 可重复读（Read-Repeatable，RR) > 读提交（Read-Comitted，RC） >读未提交 Read-Uncommited


`Read Uncommitted`（读取未提交内容）

在该隔离级别，**所有事务都可以看到其他未提交事务的执行结果**。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为**脏读（Dirty Read）**。

`Read Committed`（读取提交内容）

这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。

`Repeatable Read`（可重读）

这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，**幻读**指当用户读取某一范围的数据行时，**另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。**InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。

`Serializable`（可串行化）

这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。



### 54. 将一个链表反转（用三个指针，但是每次只反转一个）

```java
public class Solution5 {
	
	//构建链表
	public  static class ListNode{
		int val;
		ListNode nxt;
		public ListNode(int val) {
			this.val = val;
		}
		public ListNode() {}
		@Override
		public String toString() {
			return "ListNode [val=" + val + ", nxt=" + nxt + "]";
		}
	}
	
	public static ListNode reverseListNode(ListNode root){
		if(null == root)
			return null;
		ListNode prev = root;
		ListNode cur  = root.nxt;
		ListNode nxt;
		while(null != cur){
			// 反转.当前指向的下一个，反转指向当前
			 nxt = cur.nxt;
			 cur.nxt = prev;
			 // 下一次的 上一个 为这次的 当前 。
			 prev = cur;  
			 //下一次的 当前 为这次的 下一个
	         cur = nxt; 
		}
		//将原链表的头节点的下一个节点置为null
        root.nxt = null;
        //再将反转后的头节点赋给root  
        root = prev;  
		return root;
	}

	public static void main(String[] args) {
		ListNode listNode = new ListNode(1);
		listNode.nxt = new ListNode(2);
		listNode.nxt.nxt = new ListNode(3);
		listNode.nxt.nxt.nxt = new ListNode(4);
		
		System.out.println(reverseListNode(listNode));
	}
}
```

55. spring Aop的实现原理，具体说说



56. 何时会内存泄漏，内存泄漏会抛哪些异常

58. spring的注入bean的方式
59. sql语句各种条件的执行顺序，如select， where， order by， group by

	执行顺序：FROM：对FROM子句中的前两个表执行笛卡尔积（Cartesian product)(交叉联接），生成虚拟表VT1
			
			ON：对VT1应用ON筛选器。只有那些使<join_condition>为真的行才被插入VT2。
			
			OUTER(JOIN)：如果指定了OUTER JOIN（相对于CROSS JOIN 或(INNER JOIN),保留表（preserved table：左外部联接把左表标记为保留表，右外部联接把右表标记为保留表，完全外部联接把两个表都标记为保留表）中未找到匹配的行将作为外部行添加到 VT2,生成VT3.如果FROM子句包含两个以上的表，则对上一个联接生成的结果表和下一个表重复执行步骤1到步骤3，直到处理完所有的表为止。
			
			WHERE：对VT3应用WHERE筛选器。只有使<where_condition>为true的行才被插入VT4.
			
			GROUP BY：按GROUP BY子句中的列列表对VT4中的行分组，生成VT5.
			
			CUBE|ROLLUP：把超组(Suppergroups)插入VT5,生成VT6.
			
			HAVING：对VT6应用HAVING筛选器。只有使<having_condition>为true的组才会被插入VT7.
			
			SELECT：处理SELECT列表，产生VT8.
			
			DISTINCT：将重复的行从VT8中移除，产生VT9.
			
			ORDER BY：将VT9中的行按ORDER BY 子句中的列列表排序，生成游标（VC10).
			
			TOP：从VC10的开始处选择指定数量或比例的行，生成表VT11,并返回调用者。
	
	1.having只能用在group by之后，对分组后的结果进行筛选(即使用having的前提条件是分组)。 
	
	2.where肯定在group by 之前，即也在having之前。 
	
	3.where后的条件表达式里不允许使用聚合函数，而having可以。 

60. select  xx from xx where xx and xx order by xx limit xx； 如何优化

	索引
	把order by的字段放在索引的最前面

61. 四则元算写代码

### 62. 统计100G的ip文件中出现ip次数最多的100个ip

	
	ipv4 地址是一个 32 位的整数，可以用 uint 保存。

	我先设计一个哈希函数，把100个G的文件分成10000份，每份大约是 10MB，可以加载进内存了。

	例如：我设计一个简单的哈希函数是 f(ip) = ip % 10000，(ip 是个32位整数)

	那么 5 % 10000 = 5，不管 5 在哪个地方 5 % 10000 的结果都是 5，这就保证了相同的 ip 会被放在同一个子文件中，方便统计，相同的元素经过同一个哈希函数，得出的哈希值是一样的。

	那么我把100亿个 ip，都进行 ip % 10000 的操作，就完成了 100GB 文件分解成 10000个子文件的任务了。当然实际中哈希函数的选取很重要，尽量使得元素分布均匀，哈希冲突少的函数才是最好的。

	记住，我把上面这个分解的过程叫做 Map，由一台叫 master 的计算机完成这个工作。
	
	10MB 的小文件加进内存，统计出出现次数最多的那个ip

	10MB 的小文件里面存着很多 ip，他们虽然是乱序的，但是相同的 ip 会映射到同一个文件中来！

	那么可以用二叉树统计出现次数，二叉树节点保存（ip, count）的信息，把所有 ip 插入到二叉树中，如果这个 ip 不存在，那么新建一个节点, count 标记 1，如果有，那么把 count++，最终遍历一遍树，就能找出 count 最大的 ip 了。

	我把这个过程叫做 Reduce，由很多台叫 worker 的计算机来完成。

	每个 worker 至少要找出最大的前10个 ip 返回给 master，master 最后会收集到 10000 * 10 个 ip，大约 400KB，然后再找出最大的前 10 个 ip 就可以了。

	最简单的遍历10遍，每次拿个最大值出来就可以了，或者用快速排序，堆排序，归并排序等等方法，找出最大前 k 个数也行。


63. zookeeper的事物，结点，服务提供方挂了如何告知消费方
64. 5台服务器如何选出leader(选举算法)

65. 适配器和代理模式的区别
66. 读写锁
67. static加锁
68. 事务隔离级别
69. 门面模式，类图(外观模式)
70. mybatis如何映射表结构

71. 二叉树遍历

```java
    public static void preOrderTraverse(Node node) {  
        if (node == null)  
            return;  
        System.out.print(node.data + " ");  
        preOrderTraverse(node.leftChild);  
        preOrderTraverse(node.rightChild);  
    }  
```  

72. 主从复制
73. mysql引擎区别

### 74. 静态内部类加载到了哪个区？

方法区

### 75. class文件编译后加载到了哪



76. web的http请求如何整体响应时间变长导致处理的请求数变少，该如何处理？

用队列，当处理不了那么多http请求时将请求放到队列中慢慢处理，web如何实现队列

77. 线程安全的单例模式

78. 快速排序性能考虑

79. volatile关键字用法

80. 求表的size，或做数据统计可用什么存储引擎



### 81. 读多写少可用什么引擎

MyISAM

82. 假如要统计多个表应该用什么引擎



### 83. ConcurrentHashmap求size是如何加锁的，如果刚求完一段后这段发生了变化该如何处理

ConcurrentHashMap的size操作
如果我们要统计整个ConcurrentHashMap里元素的大小，就必须统计所有Segment里元素的大小后求和。Segment里的全局变量count是一个volatile变量，那么在多线程场景下，我们是不是直接把所有Segment的count相加就可以得到整个ConcurrentHashMap大小了呢？不是的，虽然相加时可以获取每个Segment的count的最新值，但是拿到之后可能累加前使用的count发生了变化，那么统计结果就不准了。所以最安全的做法，是在统计size的时候把所有Segment的put，remove和clean方法全部锁住，但是这种做法显然非常低效。

因为在累加count操作过程中，之前累加过的count发生变化的几率非常小，所以ConcurrentHashMap的做法是先尝试2次通过不锁住Segment的方式来统计各个Segment大小，如果统计的过程中，容器的count发生了变化，则再采用加锁的方式来统计所有Segment的大小。

那么ConcurrentHashMap是如何判断在统计的时候容器是否发生了变化呢？使用modCount变量，在put , remove和clean方法里操作元素前都会将变量modCount进行加1，那么在统计size前后比较modCount是否发生变化，从而得知容器的大小是否发生变化。


### 84. 1000个苹果放10个篮子，怎么放，能让我拿到所有可能的个数

	每个篮子分别放：1个 2个 4个 8个 16个 32个 64个 128个 256个 这是前九个篮子,最后一个篮子放剩下的489个

### 85. 可重入的读写锁，可重入是如何实现的？

86. 是否用过NIO

87. java的concurrent包用过没

88. sting s=new string("abc")分别在堆栈上新建了哪些对象

89. java虚拟机的区域分配，各区分别存什么

90. 分布式事务（JTA）

### 91. threadlocal使用时注意的问题

（ThreadLocal和Synchonized都用于解决多线程并发访问。但是ThreadLocal与synchronized有本质的区别。synchronized是利用锁的机制，使变量或代码块在某一时该只能被一个线程访问。

而ThreadLocal为每一个线程都提供了变量的副本，使得每个线程在某一时间访问到的并不是同一个对象，这样就隔离了多个线程对数据的数据共享。而Synchronized却正好相反，它用于在多个线程间通信时能够获得数据共享）

92. java有哪些容器(集合，tomcat也是一种容器)

93. 二分查找算法
94. myisam的优点，和innodb的区别
95. redis能存哪些类型
96. http协议格式，get和post的区别
97. 可重入锁中对应的wait和notify
98. redis能把内存空间交换进磁盘中吗(这个应该是可以的，但是那个面试官非跟我说不可以)
99. java线程池中基于缓存和基于定长的两种线程池，当请求太多时分别是如何处理的？定长的事用的队列，如果队列也满了呢？交换进磁盘？基于缓存的线程池解决方法呢？
100. synchronized加在方法上用的什么锁

101. 可重入锁中的lock和trylock的区别
102. innodb对一行数据的读会枷锁吗？不枷锁，读实际读的是副本
103. redis做缓存是分布式存的？不同的服务器上存的数据是否重复？guava cache呢？是否重复？不同的机器存的数据不同
104. 用awk统计一个ip文件中top10
105. 对表做统计时可直接看schema info信息，即查看表的系统信息
106. mysql目前用的版本
107. 公司经验丰富的人给了什么帮助？(一般boss面会问这些)
108. 自己相对于一样的应届生有什么优势
109. 自己的好的总结习惯给自己今后的工作带了什么帮助，举例为证

110. 原子类，线程安全的对象，异常的处理方式

111. 4亿个int数，如何找出重复的数（用hash方法，建一个2的32次方个bit的hash数组，每取一个int数，可hash下2的32次方找到它在hash数组中的位置，然后将bit置1表示已存在）
112. 4亿个url，找出其中重复的（考虑内存不够，通过hash算法，将url分配到1000个文件中，不同的文件间肯定就不会重复了，再分别找出重复的）
有1万个数组，每个数组有1000个整数，每个数组都是降序的，从中找出最大的N个数，N<1000

113. LinkedHashmap的底层实现
114. 类序列化时类的版本号的用途，如果没有指定一个版本号，系统是怎么处理的？如果加了字段会怎么样？
115. Override和Overload的区别，分别用在什么场景
116. java的反射是如何实现的