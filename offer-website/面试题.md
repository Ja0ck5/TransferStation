### 1. junit用法，before,beforeClass,after, afterClass的执行顺序

执行顺序是：

     @BeforeClass ，@Before，@Test，@After，@AfterClass




> @BeforeClass 在**本类加载前**执行，注意的是有关键字：static
> 
>  @Before  在执行目标测试**方法前**执行
>  
>  @Test **目标测试方法**
>  
>  @After  在执行目标测试**方法后**执行
>  
>  @AfterClass 在**本类加载后**执行，注意的是有关键字：static
  


### 2. 分布式锁

#### 分布式锁是控制分布式系统之间同步访问共享资源的一种方式。

分布式锁的方法有哪些。

1、使用数据库乐观锁，包括主键防重，版本号控制。但是这两种方法各有利弊。

使用主键冲突的策略进行防重，在并发量非常高的情况下对数据库性能会有影响，尤其是应用数据表和主键冲突表在一个库的时候，表现更加明显。其实针对是否会对数据库性能产生影响这个话题，我也和一些专业的DBA同学讨论过，普遍认可的是在MySQL数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象，比较好的办法是在程序中生产主键进行防重。

使用版本号策略 
这个策略源于mysql的mvcc机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断sql每次进行判断。


2、Zookeeper防重策略

ZooKeeper客户端curator的分布式锁实现

ZooKeeper版本的分布式锁问题相对比较来说少。
锁的占用时间限制：redis就有占用时间限制，而ZooKeeper则没有，最主要的原因是redis目前没有办法知道已经获取锁的客户端的状态，是已经挂了呢还是正在执行耗时较长的业务逻辑。而ZooKeeper通过临时节点就能清晰知道，如果临时节点存在说明还在执行业务逻辑，如果临时节点不存在说明已经执行完毕释放锁或者是挂了。由此看来redis如果能像ZooKeeper一样添加一些与客户端绑定的临时键，也是一大好事。
是否单点故障：redis本身有很多中玩法，如客户端一致性hash，服务器端sentinel方案或者cluster方案，**很难做到一种分布式锁方式能应对所有这些方案**。而ZooKeeper只有一种玩法，多台机器的节点数据是一致的，没有redis的那么多的麻烦因素要考虑。
总体上来说ZooKeeper实现分布式锁更加的简单，可靠性更高。

3 ，redis 实现分布式锁
setnx来创建一个key，如果key不存在则创建成功返回1，如果key已经存在则返回0。依照上述来判定是否获取到了锁
获取到锁的执行业务逻辑，完毕后删除lock_key，来实现释放锁
其他未获取到锁的则进行不断重试，直到自己获取到了锁

上述逻辑在正常情况下是OK的，但是一旦获取到锁的客户端挂了，没有执行上述释放锁的操作，则其他客户端就无法获取到锁了，所以在这种情况下有2种方式来解决：

- 为lock_key设置一个过期时间
- 对lock_key的value进行判断是否过期

### 3. nginx的请求转发算法，如何配置根据权重转发
####  Nginx负载均衡算法
    
	1、轮询（默认）
        每个请求按时间顺序逐一分配到不同的后端服务，如果后端某台服务器死机，自动剔除故障系统，使用户访问不受影响。
    
	2、weight（轮询权值）
        weight的值越大分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。或者仅仅为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。
    
	3、ip_hash
        每个请求按访问IP的哈希结果分配，使来自同一个IP的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的session共享问题。
    
	4、fair
        比 weight、ip_hash更加智能的负载均衡算法，fair算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间 来分配请求，响应时间短的优先分配。Nginx本身不支持fair，如果需要这种调度算法，则必须安装upstream_fair模块。
    
	5、url_hash
        按访问的URL的哈希结果来分配请求，使每个URL定向到一台后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身不支持url_hash，如果需要这种调度算法，则必须安装Nginx的hash软件包。

#### Nginx负载均衡调度状态

  在Nginx upstream模块中，可以设定每台后端服务器在负载均衡调度中的状态，常用的状态有：
    
	1、down，表示当前的server暂时不参与负载均衡
    
	2、backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的访问压力最低
    
	3、max_fails，允许请求失败的次数，默认为1，当超过最大次数时，返回proxy_next_upstream 模块定义的错误。

    4、fail_timeout，请求失败超时时间，在经历了 max_fails 次失败后，暂停服务的时间。max_fails 和 fail_timeout 可以一起使用。

#### Nginx负载均衡配置

在Nginx配置文件的HTTP层添加upstream模块

```
http {
upstream webserver {
    server 192.168.1.209:80 weight=2 max_fails=3 fail_timeout=10s;
    server 192.168.1.250:80 weight=1 max_fails=3 fail_timeout=10s;
}
server {
    listen       80;
    server_name  www.huangming.org 192.168.1.21;
    index index.html index.htm index.php index.jsp;
 
        location / {
        proxy_pass http://webserver;
        proxy_set_header Host   $host;
        proxy_set_header X-Real-IP      $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_next_upstream http_500 http_502 http_503 error timeout invalid_header;
    }
```
 
 


### 4. 用hashmap实现redis有什么问题

[http://ifeve.com/concurrenthashmap/](http://ifeve.com/concurrenthashmap/ "http://ifeve.com/concurrenthashmap/")

	（死锁，死循环，可用 ConcurrentHashmap）

多线程环境下，使用Hashmap进行put操作会引起死循环，导致CPU利用率接近100%，所以在并发情况下不能使用HashMap。



#### 效率低下的HashTable容器
  
>    HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下
>    HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，
>    其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。
>    如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，
>    并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。

#### ConcurrentHashMap的锁分段技术
    

> HashTable容器在竞争激烈的并发环境下表现出效率低下的原因，是因为所有访问HashTable的线程都必须竞争同一把锁，
> 那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，

> 那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提

> 高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，
> 
> 然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。

#### ConcurrentHashMap segment 结构

`ConcurrentHashMap` 是由 `Segment` 数组结构 和 `HashEntry` 数组结构组成。

`Segment` 是一种可重入锁 `ReentrantLock`，在 `ConcurrentHashMap` 里扮演**锁**的角色

`HashEntry` 则用于存储键值对数据。

一个 `ConcurrentHashMap` 里包含一个 `Segment` 数组，`Segment` 的结构和`HashMap` 类似，是一种**数组和链表**结构，

一个 `Segment` 里包含一个 `HashEntry` 数组，

每个 `HashEntry` 是一个链表结构的元素， 

每个 `Segment` 守护着一个 `HashEntry` 数组里的元素

当对 `HashEntry` 数组的数据进行修改时，必须首先获得它对应的 `Segment` 锁。


既然ConcurrentHashMap使用分段锁Segment来保护不同段的数据，那么在插入和获取元素的时候，必须先通过哈希算法定位到Segment。可以看到ConcurrentHashMap会首先使用Wang/Jenkins hash的变种算法对元素的hashCode进行一次再哈希。

```
private static int hash(int h) {
 
h += (h << 15) ^ 0xffffcd7d; h ^= (h >>> 10);
 
h += (h << 3); h ^= (h >>> 6);
 
h += (h << 2) + (h << 14); return h ^ (h >>> 16);
 
}
```

再哈希，其目的是为了减少哈希冲突，使元素能够均匀的分布在不同的Segment上，从而提高容器的存取效率。

#### ConcurrentHashMap的get操作
Segment的get操作实现非常简单和高效。先经过一次再哈希，然后使用这个哈希值通过哈希运算定位到segment，再通过哈希算法定位到元素，代码如下：

```java
public V get(Object key) {

       int hash = hash(key.hashCode());

       return segmentFor(hash).get(key, hash);
   }
```

get操作的高效之处在于整个get过程不需要加锁，除非读到的值是空的才会加锁重读，我们知道HashTable容器的get方法是需要加锁的，那么ConcurrentHashMap的get操作是如何做到不加锁的呢？

原因是它的get方法里将要使用的共享变量都定义成volatile，如用于统计当前Segement大小的count字段和用于存储值的HashEntry的value。

定义成volatile的变量，能够在线程之间保持可见性，能够被多线程同时读，并且保证不会读到过期的值，

但是只能被单线程写（有一种情况可以被多线程写，就是写入的值不依赖于原值），在get操作里只需要读不需要写共享变量count和value，所以可以不用加锁。

之所以不会读到过期的值，是根据java内存模型的happen before原则，对volatile字段的写入操作先于读操作，即使两个线程同时修改和获取volatile变量，get操作也能拿到最新的值，这是用volatile替换锁的经典应用场景。

```java
transient volatile int count;

volatile V value;
```

在定位元素的代码里我们可以发现定位HashEntry和定位Segment的哈希算法虽然一样，都与数组的长度减去一相与，但是相与的值不一样，定位Segment使用的是元素的hashcode通过再哈希后得到的值的高位，而定位HashEntry直接使用的是再哈希后的值。其目的是避免两次哈希后的值一样，导致元素虽然在Segment里散列开了，但是却没有在HashEntry里散列开。

```java
hash >>> segmentShift) & segmentMask//定位Segment所使用的hash算法

int index = hash & (tab.length - 1);// 定位HashEntry所使用的hash算法
```

#### ConcurrentHashMap的Put操作

由于put方法里需要对共享变量进行写入操作，所以为了线程安全，在操作共享变量时必须得加锁。Put方法首先定位到Segment，然后在Segment里进行插入操作。插入操作需要经历两个步骤，

第一步判断是否需要对Segment里的HashEntry数组进行扩容，

第二步定位添加元素的位置然后放在HashEntry数组里。

**是否需要扩容**。在插入元素前会先判断Segment里的HashEntry数组是否超过容量（threshold），如果超过阀值，数组进行扩容。值得一提的是，Segment的扩容判断比HashMap更恰当，因为HashMap是在插入元素后判断元素是否已经到达容量的，如果到达了就进行扩容，但是很有可能扩容之后没有新元素插入，这时HashMap就进行了一次无效的扩容。

如何扩容。

扩容的时候首先会创建一个两倍于原容量的数组，然后将原数组里的元素进行再hash后插入到新的数组里。为了高效ConcurrentHashMap不会对整个容器进行扩容，而只对某个segment进行扩容。

#### ConcurrentHashMap的size操作

如果我们要统计整个ConcurrentHashMap里元素的大小，就必须统计所有Segment里元素的大小后求和。Segment里的全局变量count是一个volatile变量，那么在多线程场景下，我们是不是直接把所有Segment的count相加就可以得到整个ConcurrentHashMap大小了呢？不是的，虽然相加时可以获取每个Segment的count的最新值，但是拿到之后可能累加前使用的count发生了变化，那么统计结果就不准了。所以最安全的做法，是在统计size的时候把所有Segment的put，remove和clean方法全部锁住，但是这种做法显然非常低效。

因为在累加count操作过程中，之前累加过的count发生变化的几率非常小，所以ConcurrentHashMap的做法是先尝试2次通过不锁住Segment的方式来统计各个Segment大小，如果统计的过程中，容器的count发生了变化，则再采用加锁的方式来统计所有Segment的大小。

那么ConcurrentHashMap是如何判断在统计的时候容器是否发生了变化呢？使用modCount变量，在put , remove和clean方法里操作元素前都会将变量modCount进行加1，那么在统计size前后比较modCount是否发生变化，从而得知容器的大小是否发生变化。



5. 线程的状态


5. 线程的阻塞的方式
6. sleep和wait的区别
7. hashmap的底层实现
8. 一万个人抢100个红包，如何实现（不用队列），如何保证2个人不能抢到同一个红包，可用分布式锁
9. java内存模型，垃圾回收机制，不可达算法
10. 两个Integer的引用对象传给一个swap方法在方法内部交换引用，返回后，两个引用的值是否会发现变化



11. aop的底层实现，动态代理是如何动态，假如有100个对象，如何动态的为这100个对象代理
12. 是否用过maven install。 maven test。git（make install是安装本地jar包）
13. tomcat的各种配置，如何配置docBase
14. spring的bean配置的几种方式
15. web.xml的配置
16. spring的监听器。
17. zookeeper的实现机制，有缓存，如何存储注册服务的
18. IO会阻塞吗？readLine是不是阻塞的
19. 用过spring的线程池还是java的线程池？
20. 字符串的格式化方法 （20，21这两个问题问的太低级了）



21. 时间的格式化方法
22. 定时器用什么做的
23. 线程如何退出结束
24. java有哪些锁？乐观锁 悲观锁 synchronized 可重入锁 读写锁,用过reentrantlock吗？reentrantlock与synmchronized的区别
25. ThreadLocal的使用场景
26. java的内存模型，垃圾回收机制
27. 为什么线程执行要调用start而不是直接run（直接run，跟普通方法没什么区别，先调start，run才会作为一个线程方法运行）
28. qmq消息的实现机制(qmq是去哪儿网自己封装的消息队列)
29. 遍历hashmap的三种方式

30. jvm的一些命令



31. memcache和redis的区别

32. mysql的行级锁加在哪个位置
33. ConcurrentHashmap的锁是如何加的？是不是分段越多越好
34. myisam和innodb的区别（innodb是行级锁，myisam是表级锁）
35. mysql其他的性能优化方式

36. linux系统日志在哪里看

37. 如何查看网络进程

38. 统计一个整数的二进制表示中bit为1的个数

39. jvm内存模型，java内存模型

40. 如何把java内存的数据全部dump出来

41. 如何手动触发全量回收垃圾，如何立即触发垃圾回收

42. hashmap如果只有一个写其他全读会出什么问题

43. git rebase

44. mongodb和hbase的区别

45. 如何解决并发问题
46. volatile的用途
47. java线程池（好像之前我的理解有问题）
48. mysql的binlog
49. 代理模式
50. mysql是如何实现事务的

51. 读写分离何时强制要读主库，读哪个从库是通过什么方式决定的，从库的同步mysql用的什么方式
52. mysql的存储引擎
53. mysql的默认隔离级别，其他隔离级别
54. 将一个链表反转（用三个指针，但是每次只发转一个）
55. spring Aop的实现原理，具体说说
56. 何时会内存泄漏，内存泄漏会抛哪些异常
57. 是否用过Autowire注解
58. spring的注入bean的方式
59. sql语句各种条件的执行顺序，如select， where， order by， group by
60. select  xx from xx where xx and xx order by xx limit xx； 如何优化这个（看explain）

61. 四则元算写代码

62. 统计100G的ip文件中出现ip次数最多的100个ip
63. zookeeper的事物，结点，服务提供方挂了如何告知消费方
64. 5台服务器如何选出leader(选举算法)

65. 适配器和代理模式的区别
66. 读写锁
67. static加锁
68. 事务隔离级别
69. 门面模式，类图(外观模式)
70. mybatis如何映射表结构

71. 二叉树遍历
72. 主从复制
73. mysql引擎区别
74. 静态内部类加载到了哪个区？方法区

75. class文件编译后加载到了哪

76. web的http请求如何整体响应时间变长导致处理的请求数变少，该如何处理？用队列，当处理不了那么多http请求时将请求放到队列
中慢慢处理，web如何实现队列

77. 线程安全的单例模式

78. 快速排序性能考虑

79. volatile关键字用法

80. 求表的size，或做数据统计可用什么存储引擎



81. 读多写少可用什么引擎

82. 假如要统计多个表应该用什么引擎

83. concurrenhashmap求size是如何加锁的，如果刚求完一段后这段发生了变化该如何处理

84. 1000个苹果放10个篮子，怎么放，能让我拿到所有可能的个数

85. 可重入的读写锁，可重入是如何实现的？

86. 是否用过NIO

87. java的concurrent包用过没

88. sting s=new string("abc")分别在堆栈上新建了哪些对象

89. java虚拟机的区域分配，各区分别存什么

90. 分布式事务（JTA）

91. threadlocal使用时注意的问题（ThreadLocal和Synchonized都用于解决多线程并发访问。但是ThreadLocal与synchronized有本质的区别。synchronized是利用锁的机制，使变量或代码块在某一时该只能被一个线程访问。而ThreadLocal为每一个线程都提供了变量的副本，使得每个线程在某一时间访问到的并不是同一个对象，这样就隔离了多个线程对数据的数据共享。而Synchronized却正好相反，它用于在多个线程间通信时能够获得数据共享）

92. java有哪些容器(集合，tomcat也是一种容器)

93. 二分查找算法
94. myisam的优点，和innodb的区别
95. redis能存哪些类型
96. http协议格式，get和post的区别
97. 可重入锁中对应的wait和notify
98. redis能把内存空间交换进磁盘中吗(这个应该是可以的，但是那个面试官非跟我说不可以)
99. java线程池中基于缓存和基于定长的两种线程池，当请求太多时分别是如何处理的？定长的事用的队列，如果队列也满了呢？交换进磁盘？基于缓存的线程池解决方法呢？
100. synchronized加在方法上用的什么锁

101. 可重入锁中的lock和trylock的区别
102. innodb对一行数据的读会枷锁吗？不枷锁，读实际读的是副本
103. redis做缓存是分布式存的？不同的服务器上存的数据是否重复？guava cache呢？是否重复？不同的机器存的数据不同
104. 用awk统计一个ip文件中top10
105. 对表做统计时可直接看schema info信息，即查看表的系统信息
106. mysql目前用的版本
107. 公司经验丰富的人给了什么帮助？(一般boss面会问这些)
108. 自己相对于一样的应届生有什么优势
109. 自己的好的总结习惯给自己今后的工作带了什么帮助，举例为证

110. 原子类，线程安全的对象，异常的处理方式

111. 4亿个int数，如何找出重复的数（用hash方法，建一个2的32次方个bit的hash数组，每取一个int数，可hash下2的32次方找到它在hash数组中的位置，然后将bit置1表示已存在）
112. 4亿个url，找出其中重复的（考虑内存不够，通过hash算法，将url分配到1000个文件中，不同的文件间肯定就不会重复了，再分别找出重复的）
有1万个数组，每个数组有1000个整数，每个数组都是降序的，从中找出最大的N个数，N<1000

113. LinkedHashmap的底层实现
114. 类序列化时类的版本号的用途，如果没有指定一个版本号，系统是怎么处理的？如果加了字段会怎么样？
115. Override和Overload的区别，分别用在什么场景
116. java的反射是如何实现的